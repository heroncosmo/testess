#!/usr/bin/env python3
"""
PEACOCK COSM√âTICOS - REDU√á√ÉO DE PAYLOAD DE REDE
Otimiza o payload total de 15.473 KiB, especialmente v√≠deos grandes
"""

import os
import re
import json
from pathlib import Path

def analyze_large_files():
    """Analisa arquivos grandes que contribuem para o payload"""
    print("üîç Analisando arquivos grandes...")
    
    large_files = []
    total_size = 0
    
    # Extens√µes para analisar
    target_extensions = ['.mp4', '.webm', '.jpg', '.jpeg', '.png', '.gif', '.webp', '.css', '.js']
    
    for root, dirs, files in os.walk('.'):
        # Pular diret√≥rios desnecess√°rios
        if any(skip in root for skip in ['.git', 'node_modules', '__pycache__']):
            continue
            
        for file in files:
            if any(file.lower().endswith(ext) for ext in target_extensions):
                file_path = os.path.join(root, file)
                try:
                    file_size = os.path.getsize(file_path)
                    total_size += file_size
                    
                    # Marcar arquivos grandes (>500KB)
                    if file_size > 500 * 1024:
                        large_files.append({
                            'path': file_path,
                            'size_kb': round(file_size / 1024, 1),
                            'size_mb': round(file_size / (1024 * 1024), 2),
                            'type': file.split('.')[-1].lower()
                        })
                except OSError:
                    continue
    
    # Ordenar por tamanho
    large_files.sort(key=lambda x: x['size_kb'], reverse=True)
    
    print(f"   üìä Total analisado: {round(total_size / (1024 * 1024), 1)} MB")
    print(f"   üö® {len(large_files)} arquivos grandes encontrados")
    
    return large_files, total_size

def optimize_video_delivery():
    """Otimiza entrega de v√≠deos grandes"""
    print("üé• Otimizando entrega de v√≠deos...")
    
    with open('index.html', 'r', encoding='utf-8') as f:
        content = f.read()
    
    # Encontrar v√≠deos no HTML
    video_pattern = r'<video([^>]*?)>(.*?)</video>'
    
    def optimize_video_tag(match):
        video_attrs = match.group(1)
        video_content = match.group(2)
        
        # Extrair src do v√≠deo
        src_match = re.search(r'src=[\'"]([^\'"]*)[\'"]', video_content)
        if not src_match:
            return match.group(0)
        
        video_src = src_match.group(1)
        
        # Otimiza√ß√µes para v√≠deos grandes
        optimizations = []
        
        # Preload apenas metadata
        if 'preload=' not in video_attrs:
            optimizations.append('preload="metadata"')
        
        # Adicionar poster se n√£o existir
        if 'poster=' not in video_attrs:
            poster_path = re.sub(r'\.(mp4|webm|ogg)$', '-poster.jpg', video_src)
            if os.path.exists(poster_path.lstrip('./')):
                optimizations.append(f'poster="{poster_path}"')
        
        # Adicionar loading lazy
        if 'loading=' not in video_attrs:
            optimizations.append('loading="lazy"')
        
        # Adicionar controles
        if 'controls' not in video_attrs:
            optimizations.append('controls')
        
        # Muted para autoplay
        if 'muted' not in video_attrs:
            optimizations.append('muted')
        
        opt_str = ' ' + ' '.join(optimizations) if optimizations else ''
        
        return f'<video{video_attrs}{opt_str}>{video_content}</video>'
    
    # Aplicar otimiza√ß√µes
    content = re.sub(video_pattern, optimize_video_tag, content, flags=re.DOTALL)
    
    with open('index.html', 'w', encoding='utf-8') as f:
        f.write(content)
    
    print("   ‚úÖ V√≠deos otimizados para entrega eficiente")

def implement_progressive_loading():
    """Implementa carregamento progressivo de recursos"""
    print("üìà Implementando carregamento progressivo...")
    
    progressive_script = """
<script>
// Progressive Loading Implementation
(function() {
    let loadedResources = 0;
    const totalResources = document.querySelectorAll('img, video').length;
    
    // Function to load non-critical resources progressively
    function loadProgressively() {
        const nonCriticalImages = document.querySelectorAll('img[loading="lazy"]:not(.loaded)');
        const nonCriticalVideos = document.querySelectorAll('video[loading="lazy"]:not(.loaded)');
        
        // Load images in batches
        const imageBatch = Array.from(nonCriticalImages).slice(0, 3);
        imageBatch.forEach(img => {
            if (img.dataset.src) {
                img.src = img.dataset.src;
            }
            img.classList.add('loaded');
            loadedResources++;
        });
        
        // Load videos when needed
        const videoBatch = Array.from(nonCriticalVideos).slice(0, 1);
        videoBatch.forEach(video => {
            if (video.dataset.src) {
                video.src = video.dataset.src;
                video.load();
            }
            video.classList.add('loaded');
            loadedResources++;
        });
        
        // Update progress
        const progress = Math.round((loadedResources / totalResources) * 100);
        console.log(`Loading progress: ${progress}%`);
        
        // Continue loading if there are more resources
        if (loadedResources < totalResources) {
            setTimeout(loadProgressively, 500);
        }
    }
    
    // Start progressive loading after initial load
    window.addEventListener('load', function() {
        setTimeout(loadProgressively, 1000);
    });
    
    // Load resources when user scrolls
    let scrollTimeout;
    window.addEventListener('scroll', function() {
        clearTimeout(scrollTimeout);
        scrollTimeout = setTimeout(loadProgressively, 200);
    });
})();
</script>
"""
    
    with open('index.html', 'r', encoding='utf-8') as f:
        content = f.read()
    
    # Adicionar script antes do fechamento do body
    content = re.sub(r'</body>', progressive_script + '</body>', content)
    
    with open('index.html', 'w', encoding='utf-8') as f:
        f.write(content)
    
    print("   ‚úÖ Carregamento progressivo implementado")

def optimize_css_delivery():
    """Otimiza entrega de CSS para reduzir payload inicial"""
    print("üé® Otimizando entrega de CSS...")
    
    with open('index.html', 'r', encoding='utf-8') as f:
        content = f.read()
    
    # CSS n√£o cr√≠tico que pode ser carregado depois
    non_critical_css = [
        'woocommerce',
        'widget-testimonial',
        'widget-icon',
        'fontawesome',
        'brands'
    ]
    
    # Converter CSS n√£o cr√≠tico para carregamento ass√≠ncrono
    for css_type in non_critical_css:
        pattern = f'<link([^>]*{css_type}[^>]*\\.css[^>]*)>'
        
        def make_async(match):
            link_attrs = match.group(1)
            href_match = re.search(r'href=[\'"]([^\'"]*)[\'"]', link_attrs)
            if href_match:
                href = href_match.group(1)
                return f'<link rel="preload" href="{href}" as="style" onload="this.onload=null;this.rel=\'stylesheet\'" crossorigin>'
            return match.group(0)
        
        content = re.sub(pattern, make_async, content)
    
    with open('index.html', 'w', encoding='utf-8') as f:
        f.write(content)
    
    print("   ‚úÖ CSS n√£o cr√≠tico convertido para carregamento ass√≠ncrono")

def remove_unused_resources():
    """Remove recursos n√£o utilizados"""
    print("üóëÔ∏è Removendo recursos n√£o utilizados...")
    
    with open('index.html', 'r', encoding='utf-8') as f:
        content = f.read()
    
    # Recursos que podem ser removidos
    unused_patterns = [
        # Scripts de desenvolvimento
        r'<script[^>]*astra-sites[^>]*>.*?</script>\s*',
        r'<script[^>]*template-preview[^>]*>.*?</script>\s*',
        # CSS n√£o utilizados
        r'<link[^>]*robotoslab[^>]*\.css[^>]*>\s*',
        r'<link[^>]*playfairdisplay[^>]*\.css[^>]*>\s*',
        # Scripts duplicados
        r'<script[^>]*jquery-migrate[^>]*>.*?</script>\s*',
    ]
    
    removed_count = 0
    for pattern in unused_patterns:
        matches = re.findall(pattern, content, flags=re.DOTALL)
        if matches:
            content = re.sub(pattern, '', content, flags=re.DOTALL)
            removed_count += len(matches)
    
    with open('index.html', 'w', encoding='utf-8') as f:
        f.write(content)
    
    print(f"   ‚úÖ {removed_count} recursos n√£o utilizados removidos")

def compress_inline_content():
    """Comprime conte√∫do inline"""
    print("üóúÔ∏è Comprimindo conte√∫do inline...")
    
    with open('index.html', 'r', encoding='utf-8') as f:
        content = f.read()
    
    # Remover espa√ßos em branco desnecess√°rios
    content = re.sub(r'\n\s*\n', '\n', content)  # M√∫ltiplas linhas vazias
    content = re.sub(r'>\s+<', '><', content)    # Espa√ßos entre tags
    content = re.sub(r'\s{2,}', ' ', content)    # M√∫ltiplos espa√ßos
    
    # Remover coment√°rios HTML desnecess√°rios (manter os importantes)
    content = re.sub(r'<!--(?!.*(?:Critical|Performance|Cache)).*?-->', '', content, flags=re.DOTALL)
    
    with open('index.html', 'w', encoding='utf-8') as f:
        f.write(content)
    
    print("   ‚úÖ Conte√∫do inline comprimido")

def create_resource_bundling():
    """Cria bundling de recursos pequenos"""
    print("üì¶ Criando bundling de recursos...")
    
    # Combinar CSS pequenos
    small_css_files = []
    css_dir = Path('wp-content')
    
    if css_dir.exists():
        for css_file in css_dir.rglob('*.css'):
            try:
                if css_file.stat().st_size < 10 * 1024:  # Menor que 10KB
                    small_css_files.append(css_file)
            except:
                continue
    
    if len(small_css_files) > 3:
        combined_css = ""
        for css_file in small_css_files[:5]:  # Combinar at√© 5 arquivos
            try:
                with open(css_file, 'r', encoding='utf-8') as f:
                    css_content = f.read()
                combined_css += f"/* {css_file} */\n{css_content}\n\n"
            except:
                continue
        
        if combined_css:
            os.makedirs('wp-content/uploads', exist_ok=True)
            with open('wp-content/uploads/bundled-small.css', 'w', encoding='utf-8') as f:
                f.write(combined_css)
            
            print(f"   ‚úÖ {len(small_css_files[:5])} arquivos CSS pequenos combinados")
    else:
        print("   ‚ÑπÔ∏è Poucos arquivos pequenos para combinar")

def create_payload_report():
    """Cria relat√≥rio de redu√ß√£o de payload"""
    print("üìä Gerando relat√≥rio de payload...")
    
    large_files, total_size = analyze_large_files()
    
    with open('index.html', 'r', encoding='utf-8') as f:
        content = f.read()
    
    # Analisar otimiza√ß√µes
    html_size_kb = round(len(content) / 1024, 1)
    lazy_resources = len(re.findall(r'loading="lazy"', content))
    async_css = len(re.findall(r'rel="preload".*as="style"', content))
    optimized_videos = len(re.findall(r'preload="metadata"', content))
    
    # Calcular economia estimada
    video_files = [f for f in large_files if f['type'] in ['mp4', 'webm']]
    image_files = [f for f in large_files if f['type'] in ['jpg', 'jpeg', 'png', 'webp']]
    
    estimated_savings = 0
    if video_files:
        estimated_savings += sum(f['size_kb'] for f in video_files[:2]) * 0.3  # 30% economia em v√≠deos
    if image_files:
        estimated_savings += sum(f['size_kb'] for f in image_files[:5]) * 0.2  # 20% economia em imagens
    
    report = {
        "payload_reduction_summary": {
            "total_size_mb": round(total_size / (1024 * 1024), 1),
            "html_size_kb": html_size_kb,
            "large_files_count": len(large_files),
            "largest_files": large_files[:5],
            "lazy_resources": lazy_resources,
            "async_css_files": async_css,
            "optimized_videos": optimized_videos,
            "estimated_savings_kb": round(estimated_savings, 1),
            "original_payload_kb": 15473,
            "optimizations_applied": [
                "Video delivery optimization",
                "Progressive loading implementation",
                "CSS delivery optimization",
                "Unused resource removal",
                "Inline content compression",
                "Resource bundling"
            ]
        }
    }
    
    # Salvar relat√≥rio
    with open('payload_reduction_report.json', 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    return report

def main():
    """Executa todas as otimiza√ß√µes de payload"""
    print("üéØ REDUZINDO PAYLOAD DE REDE - PEACOCK COSM√âTICOS\n")
    
    # Backup do arquivo original
    if not os.path.exists('index_payload_backup.html'):
        with open('index.html', 'r', encoding='utf-8') as f:
            content = f.read()
        with open('index_payload_backup.html', 'w', encoding='utf-8') as f:
            f.write(content)
        print("üìã Backup criado: index_payload_backup.html")
    
    # Executar otimiza√ß√µes
    large_files, total_size = analyze_large_files()
    optimize_video_delivery()
    implement_progressive_loading()
    optimize_css_delivery()
    remove_unused_resources()
    compress_inline_content()
    create_resource_bundling()
    report = create_payload_report()
    
    print("\nüéâ PAYLOAD OTIMIZADO!")
    print("üìä Resultados:")
    print(f"   üì¶ Tamanho total: {report['payload_reduction_summary']['total_size_mb']} MB")
    print(f"   üìÑ HTML: {report['payload_reduction_summary']['html_size_kb']} KB")
    print(f"   üö® Arquivos grandes: {report['payload_reduction_summary']['large_files_count']}")
    print(f"   ‚ö° Recursos lazy: {report['payload_reduction_summary']['lazy_resources']}")
    print(f"   üíæ Economia estimada: {report['payload_reduction_summary']['estimated_savings_kb']} KB")
    
    if large_files:
        print(f"\nüìÅ Maiores arquivos:")
        for file in large_files[:3]:
            print(f"   üî∏ {file['path']}: {file['size_mb']} MB")

if __name__ == "__main__":
    main()
